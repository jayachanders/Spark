{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Load Data into Spark:\n",
    "\n",
    "    Load data into Spark from CSV file.\n",
    "\n",
    "    Tasks for you:\n",
    "\n",
    "     - Load data from a JSON file.\n",
    "     - Load data from a Parquet file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"LoadCSV\").getOrCreate()\n",
    "\n",
    "# Load data from CSV file\n",
    "df_csv = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", True) \\\n",
    "    .option(\"inferSchema\", True) \\\n",
    "    .load(\"path/to/csv/file.csv\")\n",
    "\n",
    "# Show the data\n",
    "df_csv.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Data Transformations:\n",
    "\n",
    "    Use Spark to transform data.\n",
    "\n",
    "    Tasks:\n",
    "\n",
    "     - Filter data based on a specific column.\n",
    "       - How to use the filter() function to select rows based on a condition\n",
    "\n",
    "     - Join two datasets.\n",
    "       - How to use the join() function to combine the two datasets based on a common column\n",
    "       - Example of joining data where the column names are different in the two datasets\n",
    "\n",
    "     - Aggregate data using different functions.\n",
    "       - How to use the groupBy() function to group the data by a specific column\n",
    "       - Provide examples of different aggregate functions (e.g., sum(), avg(), count()) and show how to apply them to the grouped data\n",
    "\n",
    "     - Group data by a specific column.\n",
    "       - How to use the groupBy() function to group the data by a specific column\n",
    "       - Example of grouping data by multiple columns and applying aggregate functions on the grouped data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from a CSV file\n",
    "df = spark.read.csv(\"data.csv\", header=True, inferSchema=True)\n",
    "# Load data from second CSV files\n",
    "df2 = spark.read.csv(\"data2.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# 1. Filter data based on a condition\n",
    "filtered_df = df.filter(df['age'] > 25)\n",
    "\n",
    "# 2. Join the two datasets\n",
    "joined_df = df.join(df2, on=['id'], how='inner')\n",
    "\n",
    "# 3. Aggregate data using different functions\n",
    "agg_df = df.agg({'age': 'mean', 'income': 'sum'})\n",
    "\n",
    "# 4. Group data by a specific column\n",
    "grouped_df = df.groupBy('gender').count()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Spark SQL:\n",
    "\n",
    "    Use Spark SQL to interact with data.\n",
    "\n",
    "    Tasks:\n",
    "\n",
    "     - Create a temporary view from a DataFrame.\n",
    "     - Query data using Spark SQL.\n",
    "     - Perform aggregation using Spark SQL.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SparkSession:\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"SparkSQL\").getOrCreate()\n",
    "\n",
    "# Load data into a DataFrame:\n",
    "df = spark.read.csv(\"file_path.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Create a temporary view from the DataFrame:\n",
    "df.createOrReplaceTempView(\"temp_view_name\")\n",
    "\n",
    "# Query data using Spark SQL:\n",
    "result = spark.sql(\"SELECT * FROM temp_view_name WHERE column_name='value'\")\n",
    "result.show()\n",
    "\n",
    "# Perform aggregation using Spark SQL:\n",
    "\n",
    "result = spark.sql(\"SELECT column_name, AVG(column2) as avg_col2 FROM temp_view_name GROUP BY column_name\")\n",
    "result.show()\n",
    "\n",
    "# Try to modify the column names and criteria according to your data and requirements.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Machine Learning with Spark:\n",
    "\n",
    "    Use Spark to build a simple machine learning model.\n",
    "\n",
    "    Tasks:\n",
    "\n",
    "    - Load the data into Spark.\n",
    "        - Demonstrate how to load data from different sources (e.g., CSV, JSON, Parquet, etc.) into Spark.\n",
    "    \n",
    "\n",
    "    - Pre-process the data.\n",
    "        - Demonstrate how to perform common pre-processing tasks in Spark, such as handling missing values, scaling numerical features, encoding categorical features, etc.\n",
    "\n",
    "\n",
    "    - Split the data into training and testing datasets.\n",
    "        - Demonstrate how to split data into training and testing datasets using Spark.\n",
    "\n",
    "\n",
    "    - Build a simple machine learning model.\n",
    "        - Demonstrate how to use Spark's machine learning library (MLlib) to build a simple machine learning model, such as linear regression or logistic regression.\n",
    "\n",
    "\n",
    "    - Evaluate the model performance.\n",
    "        - Demonstrate how to use Spark's MLlib to evaluate the performance of a machine learning model using metrics such as accuracy, precision, recall, F1-score, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data into Spark\n",
    "df = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"path/to/data.csv\")\n",
    "\n",
    "# Pre-process the data\n",
    "from pyspark.ml.feature import Imputer, StandardScaler, OneHotEncoder, StringIndexer\n",
    "\n",
    "# Handle missing values\n",
    "imputer = Imputer(inputCols=[\"col1\", \"col2\"], outputCols=[\"col1_imputed\", \"col2_imputed\"])\n",
    "df = imputer.fit(df).transform(df)\n",
    "\n",
    "# Scale numerical features\n",
    "scaler = StandardScaler(inputCol=\"num_col\", outputCol=\"scaled_num_col\")\n",
    "df = scaler.fit(df).transform(df)\n",
    "\n",
    "# Encode categorical features\n",
    "indexer = StringIndexer(inputCol=\"cat_col\", outputCol=\"indexed_cat_col\")\n",
    "encoder = OneHotEncoder(inputCols=[\"indexed_cat_col\"], outputCols=[\"encoded_cat_col\"])\n",
    "df = indexer.fit(df).transform(df)\n",
    "df = encoder.fit(df).transform(df)\n",
    "\n",
    "# Split data into training and testing datasets\n",
    "(trainingData, testData) = df.randomSplit([0.7, 0.3])\n",
    "\n",
    "# Build a simple machine learning model\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")\n",
    "model = lr.fit(trainingData)\n",
    "\n",
    "# Evaluate the model performance\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "predictions = model.transform(testData)\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"label\")\n",
    "auc = evaluator.evaluate(predictions)\n",
    "print(\"AUC:\", auc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Spark Streaming:\n",
    "\n",
    "    Use Spark to process data in real-time.\n",
    "\n",
    "    Tasks:\n",
    "\n",
    "    - Set up a Spark Streaming application.\n",
    "        - How to create a Spark Streaming application using the SparkSession and StreamingContext classes.\n",
    "        - Explain the different configuration options available for Spark Streaming applications. FIXME\n",
    "\n",
    "    - Read data from a streaming source (e.g., Kafka, Flume, HDFS, etc.).\n",
    "        - How to read data from a streaming source using the appropriate input DStream (e.g., KafkaUtils.createDirectStream).\n",
    "\n",
    "    - Process the data in real-time (e.g., map, filter, reduceByKey, etc.).\n",
    "        - How to apply these operations to streaming data using the appropriate DStream functions.\n",
    "\n",
    "    - Write the output to a file or a database (e.g., writing to a file, writing to a database, etc.).\n",
    "        - Show how to write the output of a Spark Streaming application to the appropriate output DStream (e.g., using the writeStream function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "\n",
    "# Set up the Spark Streaming context with a batch interval of 5 seconds\n",
    "sc = SparkContext(\"local[*]\", \"StreamingExample\")\n",
    "ssc = StreamingContext(sc, 5)\n",
    "\n",
    "# Read data from a Kafka topic\n",
    "kafkaParams = {\"metadata.broker.list\": \"localhost:9092\"}\n",
    "stream = KafkaUtils.createDirectStream(ssc, [\"my_topic\"], kafkaParams)\n",
    "\n",
    "# Process the data in real-time\n",
    "words = stream.flatMap(lambda x: x[1].split(\" \"))\n",
    "word_counts = words.countByValue()\n",
    "\n",
    "# Write the output to a file\n",
    "word_counts.repartition(1).saveAsTextFiles(\"/output/directory/output\")\n",
    "\n",
    "# Start the streaming context\n",
    "ssc.start()\n",
    "ssc.awaitTermination()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Performance Tuning:\n",
    "\n",
    "    Optimize Spark performance.\n",
    "\n",
    "    Tasks:\n",
    "\n",
    "    - Optimize Spark configuration settings.\n",
    "    - Use caching to improve performance.\n",
    "    - Use broadcast variables to optimize join operations.\n",
    "    - Use partitioning to improve data processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Set up a SparkSession\n",
    "spark = SparkSession.builder.appName(\"performance-tuning\").getOrCreate()\n",
    "\n",
    "# Load some data from a file\n",
    "data = spark.read.csv(\"data.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Cache the data for faster access\n",
    "data.cache()\n",
    "\n",
    "# Use broadcast variables to optimize a join operation\n",
    "broadcast_var = spark.sparkContext.broadcast({\"key1\": \"value1\", \"key2\": \"value2\"})\n",
    "joined_data = data.join(broadcast_var.value)\n",
    "\n",
    "# Use partitioning to optimize a data processing pipeline\n",
    "partitioned_data = data.repartition(4)\n",
    "processed_data = partitioned_data.filter(\"col1 > 10\").groupBy(\"col2\").count()\n",
    "\n",
    "# Stop the SparkSession\n",
    "spark.stop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
